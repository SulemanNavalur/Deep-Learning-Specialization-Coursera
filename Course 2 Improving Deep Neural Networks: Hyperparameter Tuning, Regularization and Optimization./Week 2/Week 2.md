# Week 2

Course 2 of Deep Learning Specialization Coursera.

Course - Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization.

Week 2
 
In Week 2, we will Develop your deep learning toolbox by adding more advanced optimizations, random minibatching, and learning rate 
decay scheduling to speed up your models.

## Learning Objectives

* Apply optimization methods such as (Stochastic) Gradient Descent, Momentum, RMSProp and Adam.
* Use random minibatches to accelerate convergence and improve optimization.
* Describe the benefits of learning rate decay and apply it to your optimization.

## Content

In Week 2 we get to see,
 
* Optimization Algorithms.

## Practice Quiz

One Practice Quiz's,

* Optimization Algorithms.

## Practice Lab

One Graded programming assignment,

* Optimization Methods.
