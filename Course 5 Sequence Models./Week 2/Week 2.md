# Week 2

Course 5 of Deep Learning Specialization Coursera.

Course - Sequence Models.

Week 2
 
In Week 2, we will Natural language processing with deep learning is a powerful combination. Using word vector representations and 
embedding layers, train recurrent neural networks with outstanding performance across a wide variety of applications, including sentiment 
analysis, named entity recognition and neural machine translation.

## Learning Objectives

* Explain how word embeddings capture relationships between words.
* Load pre-trained word vectors.
* Measure similarity between word vectors using cosine similarity.
* Use word embeddings to solve word analogy problems such as Man is to Woman as King is to ______.
* Reduce bias in word embeddings.
* Create an embedding layer in Keras with pre-trained word vectors.
* Describe how negative sampling learns word vectors more efficiently than other methods.
* Explain the advantages and disadvantages of the GloVe algorithm.
* Build a sentiment classifier using word embeddings.
* Build and train a more sophisticated classifier using an LSTM.

## Content

In Week 2 we get to see,
 
* Introduction to Word Embeddings.
* Learning Word Embeddings: Word2vec & GloVe.
* Applications Using Word Embeddings.

## Practice Quiz

One Practice Quiz's,

* Natural Language Processing & Word Embeddings.

## Practice Lab

Two Graded programming assignment,

* Operations on Word Vectors - Debiasing.
* Emojify.
